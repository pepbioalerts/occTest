---
title: "Classifying Occurrence Data with *occTest*"
author: "Pep Serra-Diaz, Cory Merow, Brian Maitner, Jeremy Borderieux, Arnaud Callebaut"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Classifying Occurrence Data with *occTest*}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

# What does occTest do?

*occTest* implements a workflow to integrate, assess and control data quality of species occurrences, and is especially designed species distribution and environmental analysis and modeling. The quality control workflow follows a modular  procedure (Figure 1; see Serra-Diaz et al. 2018 and Figure 1) and performs multiple tests to identify potential errors in occurrence data using geographical and environmental information. As a second step, using the *occFilter* function allows to subset the initial dataset according to some desired accepted level of error on different test types.

The packages is based on a combination of new created functions by J.M. Serra-Diaz, C.Merow and B.Maitner, J. Borderieux and A.Callebaut, and a wrapper of functions from packages biogeo (Patterson et al. 2016) and CoordinateCleaner(Zizka et al. 2019) among others. Tests are organized in a modular fashion  in types of test (testType, e.g. all tests that identify issues in coordinates), and test blocks ('testBlocks', e.g., all tests that should be used for filtering vs. other test that characterize the accuracy).These helps including future tests and at the same time summarize results across different types of tests, and different blocks.


## Workflow overview and data needed

The main function, that implements the workflow is called *occTest*. The workflow is divided in several steps detailed below, but it consist of two main procedures: the filtering procedure and the analysis procedure (Fig.1). 

The filtering step flags occurrences that are not usable for SDMs and a strong correction of the coordinates is needed (see *biogeo* package for potential solutions), for instance locations in the sea for terrestrial species or the absence of coordinates (NA) in the record. The analysis step applies in order to assess potential errors, and how these may affect the model. These multiple tests are grouped in test blocks. These identify the nature of the error: geography, land use, environment, time and allow for a quick assessment of the dataset. 

The workflow NEEDS a MINIMUM of THREE parameters: 

1. Species name (sp.name; character)

2. Species occurrence dataset (sp.table; data.frame) 
The species occurrence data.frame contains the presence records in latitude-longitude WGS84 geographic coordinate system, with column name 'decimalLongitude' for longitude and column name 'decimalLatitude' for latitude (NOTE: these column names can be flexible and user defined) 

3. Environmental raster (r.env; rasterStack), with quantitative continuous variables such as a climate.
The environment raster is a rasterStack object that contains the climate variables intended for use in an SDM or another analysis relating species occurrence and climate. 

```{r}
#load data
library(spocc)
df <- occ(query = 'Martes martes')
occ.data <-occ2df(df)

#we change the names here but we could just change it in the tableSettings (we could see that later)
names (occ.data)[2] <- 'decimalLongitude'
names (occ.data)[3] <- 'decimalLatitude'

#load needed environmental data
library (raster)
environmentRaster = raster::getData(name='worldclim',var='bio', res=10,path = tempdir())

#test occurrences (with minimum parameters)
library(occTest)
outMartesMartes = occTest::occTest(sp.name='Martes martes', 
                                           sp.table = occ.data,
                                           r.env = environmentRaster)

```

## *occTest* output

The outputs of occTest is a  a data.frame with coordinates, and a number of additional columns that inform of tests, corrections and summary measures of tests grouped by kinds of tests.


```{r}
#check the output
class(outMartesMartes)
#number of records
nrow (outMartesMartes)
#number of variables
ncol(outMartesMartes)
#take a look at the names of the new variables
names (outMartesMartes) 
```

### Inspect the results 

The outputs of the different tests are available to users in the output table. Users will thus be able to classify or scrub the occurrences the way they want to. The names of the columns show this rationale:

[testType]_[testName]_value: numeric or logical. Shows the quantitative result of the test (sometimes the same as in the result of the _test)

[testType]_[testName]_test: logical Shows whether the occurrence passes or not the test, being T a flag for a wrong record and NA indicating that the test was not performed on that record.

[testType]_[testName]_comment: character. Shows some comments related to the specific test.

[testType]_score: numeric. Averages the results of different tests performed. It informs, for a given set of tests what is the percentatge of test that flagged the record.


Examples: 
*HumanDetection_HumanInfluence_value* gives you the score of current human influence in the record
*HumanDetection_HumanInfluence_test* gives you whether we consider the former value an error/bias (T) or not (F)
*HumanDetection_HumanInfluence_comment* gives you a commen that give further detail on the analysis. In this case that the threshold of 45 was used for the test. 
*HumanDetection_score* summarizes all the other HumanDetection tests and outputs a value from 0 to 1. A value of 0.5 would indicate that half of the tests used indicate that is an a Human signal in the record.

```{r,echo=F}
outMartesMartes[1:10,46:52]
```


Here  is the metadata to characterize the different tests and the hierarchy grouping blocks : testTypes and TestBlocks
```{r,echo=F}
colMetadata = readRDS(system.file('ext/fieldMetadata.rds',package='occTest'))
knitr::kable (colMetadata)
```

## Customizing your analysis

Many specifications may be set to the different tests, although not all will be useful in all cases. Bear in mind that a lot of tests are available, but performing more tests means more time, and also implies you need to provide more information or parameters. The version currently implemented is that of minimalSettings().



We guide you here to set specific parameters to your analysis. All the settings are accessible and can be modified. The settings for *occTest* are organized in a list of four tables.

1. tableSettings   :  providing the specifics of your input data table of occurrence records
2. analysisSetting :  providing the specifics of each test
3. writeoutSettings:  providing the specifics of which and where the outputs should be stored

### tableSettings: when you have more than xy in your table
Some occurrence data has more than x and y coordinates. If that is the case you can provide important information. The parameter *tableSettings* consists of a list with the following elements that should be identified. Note that even if you want to change only one value of the default you need to provide a full list, with each named elements.

| Named element in list    | Description     | Value  |
| ----------   |:------------------------------:| -------:|
taxonobservation.id | Name of the colum in the species table for the observation ID | character
x.field             | Name of the colum in the species table for the longitude | character
y.field             | Name of the colum in the species table for the latitude | character
t.field            | Name of the colum in the species table for the observation date | character
l.field             | Name of the colum in the species table for the locality name | character
c.field            | Name of the colum in the species table for the REPORTED COUNTRY  | character
e.field             | Name of the colum in the species table for the REPORTED ELEVATION (in meters) | character
a.field              | Name of the colum in the species table for the accuracy (in meters) of the coordinates  | character
ds.field          | Name of the colum in the species table for the source of the data  | character


### analysisSettings: when you want to select or change the tests to be performed
Do you want to set up different implementations of the analysis? Set up your own parameters. The parameter *analysisSettings* consists of a list with the following elements that should be identified. Note that even if you want to change only one value of the default you need to provide the full list, with each named elements.


| Named element in list    | Description     | Value  |
| ----------   |:------------------------------:| -------:|
\$geoSettings\$coordinate.decimal.precision | decimal precision of the coordintes | numeric
\$geoSettings\$points.proj4string | Projection to use | Proj4 character. CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")
\$rangeAnalysis\$countryfield.shapefile | spatialPolygonsDataFrame with the countries and their associated names or ISO3 codes in the dataframe | spatialPolygonsDataFrame. Defaults to rnaturalearth::ne_countries()
\$rangeAnalysis\$doRangeAnalysis | whether the user wants to do perform the analysis of comparing coordinates countries to reported native or invasive countries | logical. Dafaults to T 
\$rangeAnalysis\$excludeUnknownRanges | keep the analysis for occurrences out of the known native or alien countries? | logical. Defaults to F
\$rangeAnalysis\$doCountryRecordAnalysis | whether the user wants to do perform the analysis of comparing reported countries to reported native or invasive countries | logical. Dafaults to T 
\$rangeAnalysis\$excludeNotmatchCountry keep the analysis for occurrences when the reported country is different than the native or alien countries? | logical. Defaults to F
\$centroidAnalysis\$doCentroidDetection | whether the user wants to do perform the analysis of centroid detection | logical. Dafaults to T 
\$centroidAnalysis\$methodCentroidDetection | methods to use to detect centroids. Implemented are: 'BIEN','speciesGeoCodeR','CoordinateCleaner', 'all' | character. Dafaults to 'all' 
\$humanAnalysis\$doHyperHumanDetection | whether the user wants to do perform the analysis of human influence | logical. Dafaults to T
\$humanAnalysis\$methodHyperHumanDetection | what methods to use to detect high human influence Implemented are: 'hii','urban', 'all' | character. Dafaults to 'all' 
\$humanAnalysis\$th.human.influence | when methods are 'all' or 'hii' what is the threshold value for human influence | numeric. Dafaults to 40  
\$humanAnalysis\$ras.hii | raster of the human influence index | raster. Defaults to SEDAC human infuence index | raster
\$institutionAnalysis\$doInstitutionLocality | whether the user wants to do perform the analysis of records potentialy in biodiversity institutions | logical. Dafaults to T 
\$institutionAnalysis\$methodInstitutionLocality | methods to use to detect records  in biodiversity institutions Implemented are: 'fromBotanicLocalityName','fromCoordinates', 'all' | character. Defaults to 'all'  
\$geooutliersAnalysis\$doGeoOutliers | whether the user wants to do perform the analysis of geographical outliers  | logical. Dafaults to T 
\$geooutliersAnalysis\$methodGeoOutliers | methods to use to geographic outliers. Implemented are: 'alphaHull','distance','median','quantSamplingCorrected','grubbs, 'all' | character. Defaults to 'all'  
\$geooutliersAnalysis\$alpha.parameter | alpha parameter desired for method 'alphaHull' | numeric. Defaults to 2
\$envoutliersAnalysis\$doEnvOutliers | whether the user wants to perform the analysis of environmental outliers | logical. Dafaults to T 
\$envoutliersAnalysis\$methodEnvOutliers | methods to use to environmental outliers. Implemented are: 'bxp','grubbs', 'all' | character. Defaults to 'all'  
\$envoutliersAnalysis\$th.perc.outenv | for method 'bxp' what is the percentage of variables found as an outlier to define it as an outlier | Numeric. Defaults to 0.2
\$accuracyAnalysis\$do.geoEnvAccuracy | whether the user wants to perform the accuracy analysis | logical. Dafaults to T 
\$accuracyAnalysis\$methodGeoEnvAccuracy | methods to use for the accuracy analysis. Implemented are: 'envDeviation','percDiffCell', 'elevDiff','lattice','rasterCell','all'| character. Defaults to 'all'  
\$accuracyAnalysis\$elev.quality.threshold | for elevDiff method, threshold of the difference between reported and inferred elevation | numeric. Defaults to 100

### writeoutSettings: do you want to write the outputs?

The user can specify if outputs are to be written.

| Named element in list    | Description     | Value  |
| ----------   |:------------------------------:| -------:|
\$output.dir | Name of the output directory | character.Defaults to NULL
\$writeAllOutput | should all outptus be written | logical. Defaults to F 
\$write.simple.output | should simple output table be written | logical. Defaults to F 
\$write.simple.output | should full output table be written | logical. Defaults to F 
\$output.base.filename | what is the suffix of the output table filename? | character. Deafaults to 'QAQC'

### an example of how to retrieve and modify settings

In this example we are going to turn off tests related to geoenvironmental accuracy analysis because our data has not enough information on coordinate uncertainty. [NOTE: even under such cirumstances where you do not have that data,the result of the test would be NA and would not affect the result]

```{r}
#load minimal settings
mySettings <- minimalSettings()
#change the attribute of interest (in this case setting an analysis off)
mySettings$analysisSettings$accuracyAnalysis$do.geoEnvAccuracy = F
#add your settings to the function
out = occTest::occTest(sp.name='Martes martes', sp.table = occ.data,r.env = environmentRaster, 
                         analysisSettings = mySettings$analysisSettings,verbose = F)
```

# Example 1: The hands-off analysis (default). Classifying and filtering

We can perform a quick test with all defaults. This has a minimal number of inputs and therefore a lot of the tests cannot be performed (note the large amount of NAs in the outputs. These minimal inputs are the data.frame of species occurrence and environmental variables. Parmeter settings are minimal, and can be displayed using the minimalSettings() function

```{r}
#load packages
library(occTest)
library(spocc)

df <- occ(query = 'Pseudotsuga menziesii')
occ.data <-occ2df(df)

#we change the names here but we could just change it in the tableSettings
names (occ.data)[2] <- 'decimalLongitude'
names (occ.data)[3] <- 'decimalLatitude'

#load needed environmental data
library (raster)
renv = raster::getData(name='worldclim',var='bio', res=10,path = tempdir())

#test occurrence
library(occTest)
outDougFir = occTest(sp.name='Pseudotsuga menziesii', 
                                sp.table = occ.data,
                                r.env = renv,verbose = F)


#check outputs
knitr::kable (head(outDougFir))
```


Then we can use *occFilter* to filter the data according to different acceptance levels of issues in each record. You can have three broad thresholds for filtering data: relaxed (accept up 70% of tests showing issues), strict (20%) or majority (50%). These percentages are applied to groups of tests depending on their nature (testBlocks or testTypes). For instance, a relaxed approach to filtering will filter a record if for a given test groups (e.g. environmental data tests, geographic tests, human detection tests) more than 70% of the tests fail.

```{r}
occTest::showTests ()
```

The result is a list where the first element fitleredDataset corresponds to the filtered occurrence records and the second element summaryStats provides the summary statistics of the number of tests performed.

```{r}
#filter data
outDougFirSelec = occFilter(df = outDougFir,errorAcceptance = 'relaxed') 

#show filtered dataset
nrow (outDougFir)
nrow (outDougFirSelec[["filteredDataset"]])
```

Show summary values of tests, grouped by 
```{r}
outDougFirSelec = occFilter(df = outDougFir,errorAcceptance = 'relaxed') 

#show filtered dataset
head (outDougFirSelec[["summaryStats"]])

```
The function also allows to set your own acceptance threshold level, and change the grouping of thests (see ?occFilter for further info).

```{r}
outDougFirSelec = occFilter(df = outDougFir,errorAcceptance = 'relaxed') 
names (outDougFirSelec)
outDougFirSelec[['filteredDataset']]
```

The output statisics allow you to inspect, for different groupingTestLevels the [groupLeve]_score (the average across specific tests), [groupLevel]_rateTestPerformed (e.g. 0.14 means you performed 14% of the available tests for a given groupLevel) and  [groupLevel]__rateTesttPerformed_NtestPeformed (e.g. the total number of tests perfomed).

```{r}
head(outDougFirSelec[['summaryStats']])
```

check how the groupings changed with different levels and different threshold acceptance levels 

```{r}
outDougFirSelecstrict = occFilter (df = outDougFir, errorAcceptance = 'strict')
outDougFirSelecMajority = occFilter (df = outDougFir, errorAcceptance = 'majority')
nrow (outDougFirSelecstrict [['filteredDataset']] )
nrow (outDougFirSelecMajority [['filteredDataset']] )
nrow (outDougFirSelec[['filteredDataset']])
```

# Example 2: Making the function turn with more detailed data (and more options)

We will apply *occTest* to a dataset that has more information like the locality names, or the accuracy of the coordinates. For that we need to specify that  before running the function.

The example  is a Mediterranean species present in France, Catalonia, Spain and Andorra. But the information of the species is however variable. Some records have the date collected, other records have data on the elevation, others have information on the locality, etc...  

We will now fully use the information on that table to perform as many tests as possible. 


```{r,fig.width=6,fig.height=4,echo=FALSE}
#load packages
library(occTest)
library(ggplot2)
library(raster)
library(knitr)
library(rgdal)
```

```{r,fig.width=6,fig.height=4}
#load data
spMed = system.file('ext/SampleData/Sp3Occurrence_v4.csv',package='occTest')
occ.species.nogeoscrub <-  read.table (file = spMed,header = T,sep = ',',as.is = T)

#load the raster of the environmental variables
ras.env.f = system.file('ext/AllEnv.tif',package='occTest')
ras.env = raster::stack(ras.env.f)

######## CHECK THE INPUT DATA TABLE
str (occ.species.nogeoscrub)
```

The table data shows:
- Longitude and latitude are named as MAPX and MAPY (MAPX, MAPY)
- There is a recorded name of a country in the table (COUNTRYRECORD)
- Locality name field (LOCALITYNAME)
- Uncertainty field value (UNCERTEINTY_X_M, UNCERTAINTY_Y_M)
- Date field (DATE)

We will incorporate this information into the workflow. To do so we need to modify the minimalSettings relatedo to table

```{r}
mySettings <- occTest::minimalSettings()
mySettings$tableSettings$x.field <- 'MAPX'
mySettings$tableSettings$y.field <- 'MAPY'
mySettings$tableSettings$t.field <- 'DATE'
mySettings$tableSettings$l.field <- 'LOCALITYNAME'
mySettings$tableSettings$c.field <- 'COUNTRYRECORD'
mySettings$tableSettings$e.field <- 'ELEVATION'
mySettings$tableSettings$a.field <- c('UNCERTAINTY_X_M','UNCERTAINTY_Y_M')
```


We have a more detailed raster of elevation that will allow us to know whether the elevation recorded in our dataset is different from that inferred from the terrain model used to build the climate variables  

```{r}
#load a high resolution raster of elevations
ras.dem.f = system.file('ext/DEM.tif',package='occTest')
ras.dem = raster(ras.dem.f)
```

We have a more detailed raster of human influence than that of the report. In our of map high human influence is considered when a record is in a cell above 50. Let's modify our settings 

```{r}
#load your own human influence map
ras.humaninfluence.f = system.file('ext/HII.tif',package='occTest')
ras.humaninfluence = raster(ras.humaninfluence.f)
#specify in the settings the raster to use for human influence
mySettings$analysisSettings$humanAnalysis$methodHyperHumanDetection <-ras.humaninfluence
#specify in the settings the theshold for to identify high influence records
mySettings$analysisSettings$humanAnalysis$th.human.influence <-50
```
We also have a more detailed data on country borders 
```{r}
#load detailed country borders and shorelines
countries.pol.f = system.file('ext/CountryLevel',package='occTest')
countries.pol = rgdal::readOGR(dsn = countries.pol.f,layer='Countries_SPDF_MED')
#specify in the settings the spatial object for countries
mySettings$analysisSettings$rangeAnalysis$countries.shapefile <-countries.pol 
#specify in the settings the column name indicating the ISO 3 digit code in the spatial object
mySettings$analysisSettings$rangeAnalysis$countryfield.shapefile <- 'ISO' 
```

We also know that the species is native to Spain ('ESP') and considered invasive in France ('FRA') so we will also specify it, and flag anything that is not in these countries
```{r,warning = FALSE}
out <- occTest   (sp.name = 'My species',
                          sp.table = occ.species.nogeoscrub,
                          r.env =ras.env,
                          tableSettings = mySettings$tableSettings, 
                          analysisSettings =mySettings$analysisSettings,
                          r.dem =   ras.dem,
                          ntv.ctry = 'ESP',
                          inv.ctry = 'FRA',
                          verbose = T)


```
Now, filter the results with a rule based on majority and grouped based on 

```{r}
outFiltered_byBlock  = occFilter(df = out,errorAcceptance = 'relaxed',by = 'testBlock' ) 
outFiltered_byTestType  = occFilter(df = out,errorAcceptance = 'relaxed',by = 'testType' ) 
```


# Plotting the results

```{r fig.height = 5.5, fig.width = 7}
list_of_plots<-plot(out , occFilter_list = outFiltered_byTestType, show_plot = T)
```

